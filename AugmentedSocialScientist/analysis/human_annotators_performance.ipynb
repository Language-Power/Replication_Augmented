{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import string\n",
    "import unidecode\n",
    "from statistics import harmonic_mean\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import sys\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your current working directory (cwd) is `ReproducingAugSS/AugmentedSocialScientist/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PATHS import OFF_GS, OFF_RA_GS, OFF_MW_GS\n",
    "from PATHS import ENDOEXO_GS, ENDOEXO_RA_GS, ENDOEXO_MW_GS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy vs. Politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load annotation data\n",
    "\n",
    "# Gold Standard\n",
    "pp_ass_gs = pd.read_csv(ENDOEXO_GS)\n",
    "# Gold Standard annotated by Research assistants\n",
    "pp_ra_gs = pd.read_csv(ENDOEXO_RA_GS)\n",
    "# Gold Standard annotated by Microworkers\n",
    "pp_mw_gs = pd.read_csv(ENDOEXO_MW_GS)\n",
    "\n",
    "pp_ass_gs.labels = pp_ass_gs.labels.apply(eval)\n",
    "pp_ra_gs.labels = pp_ra_gs.labels.apply(eval)\n",
    "pp_mw_gs.labels = pp_mw_gs.labels.apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting (to ensure that the texts are ordered in the same way)\n",
    "pp_ass_gs = pp_ass_gs.sort_values(by='text').reset_index(drop=True)\n",
    "pp_ra_gs = pp_ra_gs.sort_values(by='text').reset_index(drop=True)\n",
    "pp_mw_gs = pp_mw_gs.drop_duplicates(subset=['text']).sort_values(by='text').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alignement by character\n",
    "def char_labels(texts,labels):\n",
    "    #inputs: list of text, list of labels from doccano export\n",
    "    #output: list of labels by character\n",
    "    c = 0\n",
    "    list_labels = []\n",
    "    for i,text in enumerate(texts):\n",
    "        c += len(text)\n",
    "        dic_char = {}  \n",
    "        for j in range(len(text)):\n",
    "            dic_char[j] = np.nan\n",
    "        if len(labels[i])>0:\n",
    "            for label in labels[i]:\n",
    "                for k in range(label[0],min(label[1],len(text))):\n",
    "                    dic_char[k] = label[2]\n",
    "        list_labels += list(dic_char.values())\n",
    "    assert len(list_labels)==c, f\"The length doesn't match:{len(list_labels)},{c}\"\n",
    "    return list_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy vs. Politics - Human Performance - Microworkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       other       0.47      0.70      0.56      3446\n",
      "    politics       0.57      0.58      0.57     22668\n",
      "      policy       0.74      0.69      0.72     33992\n",
      "         nan       0.12      0.59      0.20        68\n",
      "\n",
      "    accuracy                           0.65     60174\n",
      "   macro avg       0.48      0.64      0.51     60174\n",
      "weighted avg       0.66      0.65      0.65     60174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(char_labels(pp_ass_gs.text.values, pp_ass_gs.labels.values), char_labels(pp_mw_gs.text.values, pp_mw_gs.labels.values),target_names=['other','politics','policy','nan']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy vs. Politics - Human Performance - Research Assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       other       0.52      0.91      0.66      3446\n",
      "    politics       0.77      0.77      0.77     22668\n",
      "      policy       0.87      0.80      0.83     33992\n",
      "         nan       0.11      0.69      0.19        68\n",
      "\n",
      "    accuracy                           0.79     60174\n",
      "   macro avg       0.57      0.79      0.61     60174\n",
      "weighted avg       0.81      0.79      0.80     60174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(char_labels(pp_ass_gs.text.values, pp_ass_gs.labels.values), char_labels(pp_ra_gs.text.values, pp_ra_gs.labels.values),target_names=['other','politics','policy','nan']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off the Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load annotation data\n",
    "\n",
    "off_ass_gs = pd.read_csv(OFF_GS)\n",
    "off_ra_gs = pd.read_csv(OFF_RA_GS)\n",
    "off_mw_gs = pd.read_csv(OFF_MW_GS)\n",
    "\n",
    "off_ass_gs.labels = off_ass_gs.labels.apply(eval)\n",
    "off_ra_gs.labels = off_ra_gs.labels.apply(eval)\n",
    "off_mw_gs.labels = off_mw_gs.labels.apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting (to ensure that the texts are ordered in the same way)\n",
    "\n",
    "off_ass_gs['simplified_text']=off_ass_gs.text.apply(lambda x: unidecode.unidecode(x.translate(str.maketrans('', '', string.punctuation)).lower().replace(\" \",\"\")).replace(\"<\",\"\").replace(\">\",\"\"))\n",
    "off_ra_gs['simplified_text']=off_ra_gs.text.apply(lambda x: unidecode.unidecode(x.translate(str.maketrans('', '', string.punctuation)).lower().replace(\" \",\"\")).replace(\"<\",\"\").replace(\">\",\"\"))\n",
    "off_mw_gs['simplified_text']=off_mw_gs.text.apply(lambda x: unidecode.unidecode(x.translate(str.maketrans('', '', string.punctuation)).lower().replace(\" \",\"\")).replace(\"<\",\"\").replace(\">\",\"\"))\n",
    "\n",
    "\n",
    "off_ass_gs = off_ass_gs.sort_values(by='simplified_text').reset_index(drop=True)\n",
    "off_ra_gs = off_ra_gs.sort_values(by='simplified_text').reset_index(drop=True)\n",
    "off_mw_gs = off_mw_gs.sort_values(by='simplified_text').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric: by Span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each span (consecutive sequence of highlighted characters) in the gold standard, if the annotator has highlighted at least a quarter of its characters, that span is considered correctly identified by the annotator (counted as a true positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of ture positive according to the metric \"by Span\"\n",
    "def count_tp(pr, gs):\n",
    "    # Inputs: \n",
    "    #     pr: list of the spans annotated by the annotator\n",
    "    #     gs : list of the spans in the gold standard\n",
    "    # Output: the number of true positives\n",
    "    tp = 0 \n",
    "    for i, pred_labels in enumerate(pr.labels):\n",
    "        for pred_lab in pred_labels:\n",
    "            #vérifier si pred_lab est un true positive\n",
    "            for gs_lab in gs.loc[i,'labels']:\n",
    "                ## si la moitiée du span de gs est identifiée\n",
    "                if len(set(range(pred_lab[0],pred_lab[1])) & set(range(gs_lab[0],gs_lab[1]))) >= .5*(gs_lab[1]-gs_lab[0]):\n",
    "                    tp+=1\n",
    "    return tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off the Record - Human Performance - Microworkers (by Span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microworkers Precision: 0.67\n",
      "Microworkers Recall: 0.73\n",
      "Microworkers F1: 0.70\n"
     ]
    }
   ],
   "source": [
    "mw_prec = count_tp(off_mw_gs, off_ass_gs)/off_mw_gs.labels.apply(len).sum()\n",
    "mw_recall = count_tp(off_mw_gs, off_ass_gs)/off_ass_gs.labels.apply(len).sum()\n",
    "mw_f1 = harmonic_mean([mw_prec, mw_recall])\n",
    "print(f'Microworkers Precision: {mw_prec:.2f}')\n",
    "print(f'Microworkers Recall: {mw_recall:.2f}')\n",
    "print(f'Microworkers F1: {mw_f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off the Record -  Human Performance - Research Assistants (by Span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Assistants Precision: 0.85\n",
      "Research Assistants Recall: 0.86\n",
      "Research Assistants F1: 0.86\n"
     ]
    }
   ],
   "source": [
    "ra_prec = count_tp(off_ra_gs, off_ass_gs)/off_ra_gs.labels.apply(len).sum()\n",
    "ra_recall = count_tp(off_ra_gs, off_ass_gs)/off_ass_gs.labels.apply(len).sum()\n",
    "ra_f1 = harmonic_mean([ra_prec, ra_recall])\n",
    "print(f'Research Assistants Precision: {ra_prec:.2f}')\n",
    "print(f'Research Assistants Recall: {ra_recall:.2f}')\n",
    "print(f'Research Assistants F1: {ra_f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric: by Character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character-by-character comparison of annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alignement by character\n",
    "def char_labels_off(texts,labels):\n",
    "    #inputs: list of text, list of labels from doccano export\n",
    "    #output: list of labels by character\n",
    "    c = 0\n",
    "    list_labels = []\n",
    "    for i,text in enumerate(texts):\n",
    "        c += len(text)\n",
    "        dic_char = {}  \n",
    "        if type(labels[i])==list:\n",
    "            for j in range(len(text)):\n",
    "                dic_char[j] = 'autre'\n",
    "            for label in labels[i]:\n",
    "                for k in range(label[0],min(label[1],len(text))):\n",
    "                    dic_char[k] = label[2]\n",
    "        else:\n",
    "            for j in range(len(text)):\n",
    "                dic_char[j] = np.nan        \n",
    "        list_labels += list(dic_char.values())\n",
    "    assert len(list_labels)==c, f\"The length doesn't match:{len(list_labels)},{c}\"\n",
    "    return list_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off the Record - Human Performance - Microworkers (by Character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     non off       0.99      0.97      0.98    307151\n",
      "         off       0.44      0.71      0.55      9424\n",
      "\n",
      "    accuracy                           0.96    316575\n",
      "   macro avg       0.72      0.84      0.77    316575\n",
      "weighted avg       0.97      0.96      0.97    316575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(char_labels_off(off_ass_gs.text.values, off_ass_gs.labels.values), char_labels_off(off_mw_gs.text.values, off_mw_gs.labels.values),target_names=['non off','off']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off the Record - Human Performance - Research Assistants (by Character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     non off       0.99      0.99      0.99    307151\n",
      "         off       0.75      0.83      0.79      9424\n",
      "\n",
      "    accuracy                           0.99    316575\n",
      "   macro avg       0.87      0.91      0.89    316575\n",
      "weighted avg       0.99      0.99      0.99    316575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(char_labels_off(off_ass_gs.text.values, off_ass_gs.labels.values), char_labels_off(off_ra_gs.text.values, off_ra_gs.labels.values),target_names=['non off','off']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
