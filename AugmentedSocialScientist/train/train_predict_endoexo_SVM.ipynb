{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a SVM model without pre-training\n",
    "for Policy vs. Politics prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your current working directory (cwd) is `ReproducingAugSS/AugmentedSocialScientist/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PATHS import ENDOEXO_ASS, ENDOEXO_GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import randint\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "import statsmodels.stats.api as sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df.labels = df.labels.apply(eval)\n",
    "    \n",
    "    data = []\n",
    "    for i in range(df.shape[0]):\n",
    "        text = df.loc[i,'text']\n",
    "        labels = df.loc[i, 'labels']\n",
    "        for j in range(len(labels)):\n",
    "            label = labels[j]\n",
    "            data.append({'sentence':text[label[0]:label[1]],\n",
    "                         'label':label[2]}) \n",
    "            \n",
    "    data = pd.DataFrame(data)\n",
    "    dict_label = {'autre':0, 'endogène':1,'exogène':2}\n",
    "    data.label = data.label.apply(lambda x: dict_label[x])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = process('../../datasets/AugmentedSocialScientist/all_train_and_gs/endoexo/train/endoexo_train_ass.csv')\n",
    "train = process(ENDOEXO_ASS)\n",
    "gs = process(ENDOEXO_GS)\n",
    "\n",
    "gs['label_per_char'] = gs.apply(lambda row: [row['label']]*len(row['sentence']) ,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run 100 randomly training of a SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae5d3f0dfba4d8f81e73e5b3a98f6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_scores = []\n",
    "\n",
    "for n_exp in tqdm(range(100)):\n",
    "    seed = randint(0,10000)\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', SGDClassifier(loss='hinge',penalty='l2',alpha=1e-3, random_state=seed))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    text_clf.fit(train.sentence.values, train.label.values)\n",
    "    pred = text_clf.predict(gs.sentence.values)\n",
    "\n",
    "    gs['pred'] = pred\n",
    "\n",
    "    gs['pred_per_char'] = gs.apply(lambda row: [row['pred']]*len(row['sentence']) ,axis=1)\n",
    "\n",
    "\n",
    "    # character-level scores\n",
    "    scores = precision_recall_fscore_support(gs['label_per_char'].sum(), gs['pred_per_char'].sum())\n",
    "\n",
    "    score_type = ['precision','recall','f1']\n",
    "    cat = ['other','politics','policy']\n",
    "\n",
    "    dict_scores = {'random_state':seed}\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            dict_scores[score_type[i]+'_'+cat[j]] = scores[i][j]\n",
    "            \n",
    "    list_scores.append(dict_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(list_scores)\n",
    "df_scores['f1 policy vs. politics'] = df_scores[['f1_politics','f1_policy']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv('./train/svm_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing performance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.read_csv('./train/svm_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision_other</th>\n",
       "      <th>precision_politics</th>\n",
       "      <th>precision_policy</th>\n",
       "      <th>recall_other</th>\n",
       "      <th>recall_politics</th>\n",
       "      <th>recall_policy</th>\n",
       "      <th>f1_other</th>\n",
       "      <th>f1_politics</th>\n",
       "      <th>f1_policy</th>\n",
       "      <th>f1 policy vs. politics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.603744</td>\n",
       "      <td>0.708652</td>\n",
       "      <td>0.047232</td>\n",
       "      <td>0.598130</td>\n",
       "      <td>0.781423</td>\n",
       "      <td>0.090184</td>\n",
       "      <td>0.600852</td>\n",
       "      <td>0.743229</td>\n",
       "      <td>0.672040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5% CI</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.602950</td>\n",
       "      <td>0.707854</td>\n",
       "      <td>0.046582</td>\n",
       "      <td>0.595681</td>\n",
       "      <td>0.780021</td>\n",
       "      <td>0.088992</td>\n",
       "      <td>0.599574</td>\n",
       "      <td>0.742708</td>\n",
       "      <td>0.671314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97.5% CI</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.604538</td>\n",
       "      <td>0.709450</td>\n",
       "      <td>0.047881</td>\n",
       "      <td>0.600578</td>\n",
       "      <td>0.782826</td>\n",
       "      <td>0.091376</td>\n",
       "      <td>0.602131</td>\n",
       "      <td>0.743750</td>\n",
       "      <td>0.672767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          precision_other  precision_politics  precision_policy  recall_other  \\\n",
       "mean                  1.0            0.603744          0.708652      0.047232   \n",
       "2.5% CI               1.0            0.602950          0.707854      0.046582   \n",
       "97.5% CI              1.0            0.604538          0.709450      0.047881   \n",
       "\n",
       "          recall_politics  recall_policy  f1_other  f1_politics  f1_policy  \\\n",
       "mean             0.598130       0.781423  0.090184     0.600852   0.743229   \n",
       "2.5% CI          0.595681       0.780021  0.088992     0.599574   0.742708   \n",
       "97.5% CI         0.600578       0.782826  0.091376     0.602131   0.743750   \n",
       "\n",
       "          f1 policy vs. politics  \n",
       "mean                    0.672040  \n",
       "2.5% CI                 0.671314  \n",
       "97.5% CI                0.672767  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recap = pd.concat([pd.DataFrame(df_scores.mean()).transpose(), df_scores.agg(lambda g: sms.DescrStatsW(g).tconfint_mean())])\n",
    "recap = recap.drop('random_state', axis=1)\n",
    "recap.index = ['mean', '2.5% CI', '97.5% CI']\n",
    "\n",
    "recap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
