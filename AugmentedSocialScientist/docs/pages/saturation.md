[Home](../../../README.md) | [Data](../../../datasets/README.md) | [Package](../../../Core/README.md) | [Performance Scores](./saturation.md) | [Predictions](./train_predict.md) | [Results](./analysis.md)
# Performance Scores



## 1. Aim

The aim of the ```saturation``` subrepository is to run a series of learning experiments on our two tasks, Policy vs. Politics classification and Off-the-record detection.

A *learning experiment* takes three inputs: 

- A training set
- A gold standard / evaluation set
- The model's hyperparameters

And has one output:

- A Json file containing the performances of the model trained on the given training set with the given hyperparameters set, and evaluated against a given gold standard.

In our setting, we want to obtain a sample-efficiency curve showing the average performance of a model with regards to the proportion of the data used to train it. In other words, we want to know how the model performs with 10%, 50%, and 100% of the training dataset, in order to see how much annotation is really needed. 

Hyperparameters are fixed for all the experiments. They were previously determined via trial and error first, and then confirmed with a Grid Search algorithm, not presented here for the sake of clarity.

Hyperparameters are written in the file ```VARS.py```


## 2. Subrepository structure


The repo is structured around the following files :

- ```VARS.py``` Contains hyperparameters 
- ```PATHS.py``` Contains the paths to datasets
- ```saturation/off.py``` & ```saturation/endoexo.py``` are the main files designing the experiments for Off-the-record task and Policy vs. Politics (endoexo everywhere in the repo) respectively.
- ```saturation/off``` and ```saturation/endoexo``` are very similar subrepos containing:
  - A ```trial_off.py```(same for "endoexo") script which imports abstract functions from ```off.py```, loads dataset paths, hyperparameters, runs the experiments and logs them in :
  - ```logs.jsonl``` contains the logs for every experiment generated by any run of ```trial_off.py```
  - ```ct.sh``` is simply a shell script lauching ```trial_off.py```

- ```metascript.sh``` is our script which launches the ```ct.sh```scripts on our Univa Grid Engine cluster. This might not be of any use outside Univa Grid Engine.



## 3. Running the scripts

### 3.1 Requirements 

- Having a recent GPU 
- Installing the 'Core' package (see [installation guide](../../../README.md))

### 3.2 Running an experiment 

Off-the record:

 ```bash
conda activate AugmentedSocialScientist
python AugmentedSocialScientist/saturation/off/trial_off.py
 ```
Policy vs. Politics:
 ```bash
conda activate AugmentedSocialScientist
python AugmentedSocialScientist/saturation/endoexo/trial_endoexo.py
 ```

**VERY IMPORTANT:**
An experiment trains one model for any percent of data present in ```VARS.py```. If, in ```VARS.py```, the variable is set to:
```python
 PERCENT_OF_DATA = [0.1, 0.5, 1.0]
 ```
 then the python script will train 3 models: one with a random subsampling containing 10% of the dataset, one with a random subsampling containing 50% of the dataset, and one containing 100% of the dataset.

This script does not log the models in order to avoid saturating the computer's memory with model logs. Performances & hyperparameters are logged in ```saturation/off/logs.jsonl```. A line in ```saturation/off/logs.jsonl``` corresponds to one trained model.

Our experiments presented in the article are logged in ```logs_final.jsonl``` in order to avoid overwriting the file while testing the scripts.

As the subsampling is random, we runned 5 to 20 experiments for each task, which means we runned 5 to 20 times this command:
 ```bash
python AugmentedSocialScientist/saturation/off/trial_off.py
 ```

## 4. Troubleshooting:

If you have trouble running this command:
 ```bash
python AugmentedSocialScientist/saturation/off/trial_off.py
 ```
make sure your current working directory is ```ReproducingAugSS```
